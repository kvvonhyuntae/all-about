#ML_start

##1. scikit-learn
https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html

<br>모델은 class로 구성되어 있으며 세가지의 주요한 추상 class로 부터 만들어진다.</br>

1. Estimator 추론기
   - fit(X, y) 메소드와 getparams, setparams
2. Transformer 변환기
   - transform 메소드를 갖는다.
   - transform 이전에 fit이 선행 되어야 한다.
   - 모든 transformer는 Estimator이다.
3. Predictor 예측기
   - predict(X) 메소드를 가지고 있다.
   
## 2. 모델링 과정
1. 분석 기획
   - 목적
   - 기간/조직/비용/수집 데이터 식별/정의
2. 데이터 수집 및 저장
   - '[train / validation] / test'로 데이터 셋 분리
   - train : 모델을 학습하는데 사용, w 학습
   - validation : 튜닝 파라미터를 선택
   - test : 모델의 실제 성능을 측정
   
---------

```python
import sklearn
from sklearn.datasets import load_breast_cancer #유방암 데이터
from sklearn.model_selection import train_test_split #train, test 데이터 분리
from sklearn.tree import DecisionTreeClassifier #분류기

data = load_breast_cancer() #유방암 데이터
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=2021)

model = DecisionTreeClassifier(criterion='entropy')
model
```
- DecisionTreeClassifier()가 sklearn에서 제공하는 의사 결정 트리 학습 api이며, criterion='entropy'는 엔트로피를 불순도 계산 방법으로 적용한다는 의미이다. 
- sklearn의 DecisionTreeClassifier에서 지원하는 불순도 계산 방법은 지니 인덱스와 엔트로피이다.
- 지니 인덱스로 적용하고자 한다면 criterion='gini'로 입력하면 된다.

## 3.지니 VS 엔트로피
1. gini(지니)
   - 정답이 아닌 다른 라벨이 뽑혀 나올 확률


 2. entropy(엔트로피) 
    - 정보 이론에서 탄생한 개념으로 정보량을 의미한다. 
    - 엔트로피가 높다는 것은 정보가 많다는 것이고 정보가 많다는 것은 확률이 낮다는 뜻이다.
    - 엔트로피는 로그를 취하기 때문에 지니 인덱스보다 연산이 무것운 것이 특징이다. 
    - 또한 단순 확률을 사용하는 지니와는 다른 값을 갖게 된다.


3. information Gain(정보 이득량)
   - parent noe - child node // 부모 노드에서 자식 노드의 정확도를 뺀 것, 
   - information gain이 클수록 순도가 더 높게 나뉘었다고 판단할 수 있다.    
   - 때문에 information gain이 값이 없을때 tree는 분기(split)를 멈춘다.


4. 지니 VS 엔트로피
    - 보통의 경우 지니보다는 엔트로피의 성능이 더 좋게 나옴
    - 지니를 사용했을때 연산 속도가 빠르다. 
    - 따라서 시간을 투자해서라도 좋은 성능을 원한다면 엔트로피, 준수한 성능과 빠른 연산 속도를 원한다면 지니를 사용한다.
---
```python
data.data.shape
#출력_(569, 30)
```
- 입력데이터 X는 data.data에 존재한다.
```python
data.data.mean(axis=0)
data.data.std(axis=1)
```
- 데이터의 평균 값의 분포가 다양하기 때문에 스케일링이 필요하다. 다만 tree 기반 모델은 스케일링을 필요로 하지 않는다 .  

```python
print(data.DESCR)
#출력
# .. _breast_cancer_dataset:
# 
# Breast cancer wisconsin (diagnostic) dataset
# --------------------------------------------
# 
# **Data Set Characteristics:**
# 
#     :Number of Instances: 569
# 
#     :Number of Attributes: 30 numeric, predictive attributes and the class
# 
#     :Attribute Information:
#         - radius (mean of distances from center to points on the perimeter)
#         - texture (standard deviation of gray-scale values)
#         - perimeter
#         - area
#         - smoothness (local variation in radius lengths)
#         - compactness (perimeter^2 / area - 1.0)
#         - concavity (severity of concave portions of the contour)
#         - concave points (number of concave portions of the contour)
#         - symmetry
#         - fractal dimension ("coastline approximation" - 1)
# 
#         The mean, standard error, and "worst" or largest (mean of the three
#         worst/largest values) of these features were computed for each image,
#         resulting in 30 features.  For instance, field 0 is Mean Radius, field
#         10 is Radius SE, field 20 is Worst Radius.
# 
#         - class:
#                 - WDBC-Malignant
#                 - WDBC-Benign
# 
#     :Summary Statistics:
# 
#     ===================================== ====== ======
#                                            Min    Max
#     ===================================== ====== ======
#     radius (mean):                        6.981  28.11
#     texture (mean):                       9.71   39.28
#     perimeter (mean):                     43.79  188.5
#.
#.
#.
#      rian. Machine learning techniques
#      to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 
#      163-171.
```
- dataset에 대한 정보 출력

```python
data.feature_names
#출력
# array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',
#       .... , 'worst concave points',
#        'worst symmetry', 'worst fractal dimension'], dtype='<U23')
```
- feature 데이터의 Column 출력

```python
data.target_names
#출력
#array(['malignant', 'benign'], dtype='<U9')
```
- target 데이터의 Column 출력
-----

```python
import numpy as np
uniq, freq = np.unique(data.target, return_counts=True)
uniq, freq
#출력
#(array([0, 1]), array([212, 357]))
```
```python
import pandas as pd
pd.Searise(data.target).value_counts()
#출력
# 1    357
# 0    212
# dtype: int64
```
- 0과 1의 빈도 계산하기
----

```python
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred
```
- 모델을 학습데이터로 학습하고, 테스트 데이터로 애측을 수행하여 예측값(y_pred)를 생성한다.


```python
model.score(X_test, y_test)
# 출력
# 0.958041958041958
model.score(X_train, y_train)
# 출력
# 1.0
```
- score를 이용하여 모델의 정확도를 측정할 수 있다. 위 데이터의 경우 train 데이터와 test 데이터의 차이가 많이 나서 test에서의 실제 정확도는 만족스럽지 않음
----
- 모델의 학습 내용은 tree에 저장되어있다.
```
!pip install graphviz
```
- graphiz 설치방법
````python
tree = model.tree_
````
```python
import graphiz
from sklearn.tree import export_graphviz, plot_tree
```
- 모델의 학습 내용은 tree에 저장되어 있음.
```python
plot_tree(model)
#출력
```
![](../../../Users/KVVONH~1/AppData/Local/Temp/download.png)
- plot_tree 함수로 split point와 tree를 이미지로 출력할 수 있다.
----
```python
dot_data = exprot_graphviz(model, out_file=None,
                            feature_names=data.feature_names,
                            filled=True, rounded=True,
                            special_characters=True)
graph = graphviz.Source(dot_data)
graph
```
![](file:///C:/workspace/output.svg)

<br> 맨 처음 작업전 해야할 사항 </br>

1. pip install graphviz // graphviz 설치
2. graphviz 사이트에서 설치 파일을 다운로드 후 설치
3. 환경 변수에서 graphviz 설치 폴더의 bin 폴더를 path에 등륵(설치옵션)
4. pc 재부팅
----
## Scaler
````python
X_tr_scaled = scaler.fit_transform(X_train)
X_tr_scaled = scaler.fit(X_train).transform(X_train)
````
- fir_transform 메소드는 학습과 변환을 한번에 수행한다. 
- 위 2개 코드는 같은 결과를 출력한다.

```python
from sklearn.preprocessing import StandardScaler

print(X_train[:1])

scaler = StandardScaler()
X_tr_scaled = scaler.fit_transform(X_train)
pirnt(X_tr_scaled[:1])
X_tst_scaled = scaler.transform(X_test)
```
```python
means, stds = X_trains.mean(axis=0),X_train.std(axis=0)
((X_train - means)/stds)[:1]
```
```python
X_tr_scaled.mean(0)
```
```python
m2 = DecisionTreeClassifier(criterion='entropy', maxdepth=5)
m2.fit(X_tr_scaled, y_train)
m2.score(X_tst_scaled, y_test)
```
```python
model.score(X_train, y_train)
```
- StandardSclaer는 평균을 0, 표준 분포화 하는 스케일러이다.
- 스케일된 데이터로 학습한 모델로 스케일링이 안된 데이터를 평가햐면 엉뚱한 결과가 나온다.
